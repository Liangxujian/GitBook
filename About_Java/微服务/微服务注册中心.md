# 微服务注册中心

> 该笔记参考自博客：[ZooKeeper、Eureka、Consul 、Nacos，微服务注册中心怎么选？](https://mp.weixin.qq.com/s/z9EhvpobpYvvmpgXNu3llg)

## 前言

服务注册中心本质上是为了解耦【服务提供者】和【服务消费者】。对于任何一个微服务，原则上都应存在或者支持多个提供者，这是由微服务的分布式属性决定的。

更进一步，为了支持弹性扩缩容特性，一个微服务的提供者的数量和分布往往是动态变化的，也是无法预先确定的。

因此，原本在单体应用阶段常用的静态LB机制就不再适用了，需要引入额外的组件来管理微服务提供者的注册与发现，而这个组件就是服务注册中心。

## CAP理论

### C - Consistency（一致性）

所有节点在同一时间具有相同的数据，即对于某一客户端来说，读操作保证能够返回最新的写操作结果

* 弱一致性：能容忍后续的部分或者全部访问不到。
* 强一致性：对于关系型数据库，要求更新过的数据能被后续的访问都能看到。
* 最终一致性：经过一段时间后要求能访问到更新后的数据。

### A - Availability（可用性）

每个请求不管成功或者失败都有响应（非故障的节点在合理的时间内返回合理的响应）

### P - Partition（分区容忍性）

分布式系统在遇到某节点或网络分区故障的时候，仍然能够对外提供满足一致性或可用性的服务。

## 服务注册中心解决方案

> 设计或者选择一个服务注册中心，首先要考虑的就是服务注册与发现机制。

现今主流的服务注册中心解决方案大致可归为三类：

* 应用内：直接集成到应用中，依赖于应用自身完成服务的注册与发现【如：Netflix 的 Eureka；ZooKeeper；Etcd 等】
* 应用外：把应用当成黑盒，通过应用外的某种机制将服务注册到注册中心，最小化对应用的侵入性【如：HashiCorp 的 Consul；Airbnb 的 SmartStack 等】
* DNS：将服务注册为 DNS 的 SRV 记录，严格来说，是一种特殊的应用外注册方式，SkyDNS 是其中的代表【DNS 存在固有的缓存缺陷】

除了基本的服务注册与发现机制，从开发和运维角度，至少还要考虑如下五个方面：

测活：服务注册之后，如何对服务进行测活以保证服务的可用性？

负载均衡：当存在多个服务提供者时，如何均衡各个提供者的负载？

集成：在服务提供端或者调用端，如何集成注册中心？

运行时依赖：引入注册中心之后，对应用的运行时环境有何影响？

可用性：如何保证注册中心本身的可用性，特别是消除单点故障？

## 主流服务注册中心产品

|                  | ZooKeeper | Eureka     | Consul            | Nacos                     | CoreDNS    |
| :--------------- | :-------- | :--------- | :---------------- | :------------------------ | ---------- |
| 一致性协议       | CP        | AP         | CP                | CP+AP                     | —          |
| 健康检查         | KeepAlive | ClientBeat | TCP/HTTP/gRPC/Cmd | TCP/HTTP/MYSQL/ClientBeat | —          |
| 负载均衡策略     | —         | Ribbon     | Fabio             | 权重/metadata/Selector    | RoundRobin |
| 雪崩保护         | 无        | 有         | 无                | 有                        | 无         |
| 自动注销实例     | 支持      | 支持       | 支持              | 支持                      | 不支持     |
| 访问协议         | TCP       | HTTP       | HTTP/DNS          | HTTP/DNS                  | DNS        |
| 监听支持         | 支持      | 支持       | 支持              | 支持                      | 不支持     |
| 多数据中心       | 不支持    | 支持       | 支持              | 支持                      | 不支持     |
| 跨注册中心同步   | 不支持    | 不支持     | 支持              | 支持                      | 不支持     |
| Spring Cloud集成 | 支持      | 支持       | 支持              | 支持                      | 不支持     |
| Dubbo集成        | 支持      | 不支持     | 支持              | 支持                      | 不支持     |
| K8S集成          | 不支持    | 不支持     | 支持              | 支持                      | 支持       |

### Apache ZooKeeper【CP】

与 Eureka 有所不同，Apache ZooKeeper 在设计时就谨遵 CP 原则，即任何时候对 ZooKeeper 的访问请求能得到一致的数据结果，同时系统对网络分割具备容错性，但是 ZooKeeper 不能保证每次服务请求都是可达的。

[从 ZooKeeper 的实际应用情况来看，在使用 ZooKeeper 获取服务列表时，如果此时的 ZooKeeper 集群中的 Leader 宕机了，该集群就要进行 Leader 的选举，又或者 ZooKeeper 集群中半数以上服务器节点不可用（例如有三个节点，如果节点一检测到节点三挂了 ，节点二也检测到节点三挂了，那这个节点才算是真的挂了），那么将无法处理该请求。所以说，ZooKeeper 不能保证服务可用性。](http://mp.weixin.qq.com/s?__biz=MzI3ODcxMzQzMw==&mid=2247506518&idx=2&sn=d5087e351d69e4968f0f27c9b375fcb5&chksm=eb505f60dc27d676ff5b8280ce8207a6bdc807000484c38385f7ccfff5cb34c250a99242c4ed&scene=21#wechat_redirect)

当然，在大多数分布式环境中，尤其是涉及到数据存储的场景，数据一致性应该是首先被保证的，这也是 ZooKeeper 设计谨遵 CP 原则的另一个原因。

但是对于服务发现来说，情况就不太一样了，针对同一个服务，即使注册中心的不同节点保存的服务提供者信息不尽相同，也并不会造成灾难性的后果。

因为对于服务消费者来说，能消费才是最重要的，消费者虽然拿到可能不正确的服务实例信息后尝试消费一下，也要胜过因为无法获取实例信息而不去消费，导致系统异常要好（淘宝的双十一，京东的 618 就是谨遵 AP 的最好参照）。

当 master 节点因为网络故障与其他节点失去联系时，剩余节点会重新进行 leader 选举。问题在于，选举 leader 的时间太长，30~120s，而且选举期间整个 zk 集群都是不可用的，这就导致在选举期间注册服务瘫痪。

在云部署环境下， 因为网络问题使得 zk 集群失去 master 节点是大概率事件，虽然服务能最终恢复，但是漫长的选举事件导致注册长期不可用是不能容忍的。

### Spring Cloud Eureka【AP】

Spring Cloud Netflix 在设计 Eureka 时就谨遵 AP 原则。（尽管现在2.0发布了，但是由于其闭源的原因 ，但是目前 Eureka 1.x 仍然是比较活跃的）

Eureka Server 也可以运行多个实例来构建集群，解决单点问题，但不同于 ZooKeeper 的选举 leader 的过程，Eureka Server 采用的是 Peer to Peer 对等通信。这是一种去中心化的架构，无 master/slave 之分，每一个 Peer 都是对等的。在这种架构风格中，节点通过彼此互相注册来提高可用性，每个节点需要添加一个或多个有效的 serviceUrl 指向其他节点。每个节点都可被视为其他节点的副本。

在集群环境中如果某台 Eureka Server 宕机，Eureka Client 的请求会自动切换到新的 Eureka Server 节点上，当宕机的服务器重新恢复后，Eureka 会再次将其纳入到服务器集群管理之中。当节点开始接受客户端请求时，所有的操作都会在节点间进行复制（replicate To Peer）操作，将请求复制到该 Eureka Server 当前所知的其它所有节点中。

当一个新的 Eureka Server 节点启动后，会首先尝试从邻近节点获取所有注册列表信息，并完成初始化。Eureka Server 通过 getEurekaServiceUrls() 方法获取所有的节点，并且会通过心跳契约的方式定期更新。

默认情况下，如果 Eureka Server 在一定时间内没有接收到某个服务实例的心跳（默认周期为30秒），Eureka Server 将会注销该实例（默认为90秒， eureka.instance.lease-expiration-duration-in-seconds 进行自定义配置）。

当 Eureka Server 节点在短时间内丢失过多的心跳时，那么这个节点就会进入自我保护模式，详细阅读：[Eureka 自我保护机制实战分析](http://mp.weixin.qq.com/s?__biz=MzI3ODcxMzQzMw==&mid=2247489571&idx=2&sn=9d1ecae8501d6273300c55dc7439f98b&chksm=eb539d15dc241403d9e03d1d064ebb63d930ed506ec099bb938e4e0f09a1a969b82f7a8df593&scene=21#wechat_redirect)。

Eureka 的集群中，只要有一台 Eureka 还在，就能保证注册服务可用（保证可用性），只不过查到的信息可能不是最新的（不保证强一致性）。除此之外，Eureka 还有一种自我保护机制，如果在 15 分钟内超过 85% 的节点都没有正常的心跳，那么 Eureka 就认为客户端与注册中心出现了网络故障，此时会出现以下几种情况：

1. Eureka 不再从注册表中移除因为长时间没有收到心跳而过期的服务；
2. Eureka 仍然能够接受新服务注册和查询请求，但是不会被同步到其它节点上（即保证当前节点依然可用）；
3. 当网络稳定时，当前实例新注册的信息会被同步到其它节点中；

因此，Eureka 可以很好的应对因网络故障导致部分节点失去联系的情况，而不会像 ZooKeeper 那样使得整个注册服务瘫痪。

#### Eureka 的 AP

1. 服务注册相对要快，因为不需要等注册信息 replicate 到其他节点，也不保证注册信息是否 replicate 成功
2. 当数据出现不一致时，虽然 A、B 上的注册信息不完全相同，但每个 Eureka 节点依然能够正常对外提供服务，这会出现查询服务信息时如果请求 A 查不到，但请求 B 就能查到。如此保证了可用性但牺牲了一致性。

### Consul

Consul 是 HashiCorp 公司推出的开源工具，用于实现分布式系统的服务发现与配置。Consul 使用 Go 语言编写，因此具有天然可移植性（支持 Linux、windows 和 Mac OS X）。

Consul 内置了服务注册与发现框架、分布一致性协议实现、健康检查、Key/Value 存储、多数据中心方案，不再需要依赖其他工具（比如 ZooKeeper 等），使用起来也较为简单。

Consul 遵循 CAP 原理中的 CP 原则，保证了强一致性和分区容错性，且使用的是 Raft 算法，比 ZooKeeper 使用的 Paxos 算法更加简单。虽然保证了强一致性，但是可用性就相应下降了，例如服务注册的时间会稍长一些，因为 Consul 的 Raft 协议要求必须过半数的节点都写入成功才认为注册成功 ；在 leader 挂掉了之后，重新选举出 leader 之前会导致 Consul 服务不可用。

Consul 本质上属于应用外的注册方式，但可以通过 SDK 简化注册流程。而服务发现恰好相反，默认依赖于 SDK，但可以通过 Consul Template（下文会提到）去除 SDK 依赖。

#### Consul Template

Consul，默认服务调用者需要依赖 Consul SDK 来发现服务，这就无法保证对应用的零侵入性。

所幸通过 Consul Template，可以定时从 Consul 集群获取最新的服务提供者列表并刷新 LB 配置（比如 Nginx 的 upstream），这样对于服务调用者而言，只需要配置一个统一的服务调用地址即可。

#### Consul 的强一致性（C）

1. 服务注册相比 Eureka 会稍慢一些。因为 Consul 的 Raft 协议要求必须过半数的节点都写入成功才认为注册成功
2. Leader 挂掉时，重新选举期间整个 Consul 不可用。保证了强一致性但牺牲了可用性。

#### Consul VS Eureka

Eureka 就是个 servlet 程序，跑在 servlet 容器中；而 Consul 则是 go 编写而成。

### Nacos

[Nacos 是阿里开源的，Nacos 支持基于 DNS 和基于 RPC 的服务发现。在 Spring Cloud 中使用 Nacos，只需要先下载 Nacos 并启动 Nacos server，Nacos 只需要简单的配置就可以完成服务的注册发现。](http://mp.weixin.qq.com/s?__biz=MzI3ODcxMzQzMw==&mid=2247517254&idx=1&sn=20a53224480233453582aa37e6bcfdab&chksm=eb500170dc2788663e772dc62340a37a273c50f6b0fcfccf91eee0bd114527c38a8b98051631&scene=21#wechat_redirect)

Nacos 除了服务的注册发现之外，还支持动态配置服务。动态配置服务可以让您以中心化、外部化和动态化的方式管理所有环境的应用配置和服务配置。动态配置消除了配置变更时重新部署应用和服务的需要，让配置管理变得更加高效和敏捷。配置中心化管理让实现无状态服务变得更简单，让服务按需弹性扩展变得更容易。

一句话概括就是：【Nacos】 = 【Spring Cloud注册中心】 + 【Spring Cloud配置中心】。

